---
title: "Group E 常態假設的重要性"
output: rmdformats::readthedown
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
set.seed(5555)
```
<style>
h3 {
    color: #000000;
}
</style>

# 報告介紹
## 實驗設計：  
用何方法可更好的設計一個實驗，屬於方法論的範疇。  
由於任何實驗都會受到外來環境影響，如何設計實驗，使外來環境的變化能夠對實驗造成最小的影響，就是實驗規劃的目的。  

## 為何做實驗設計？  
降低實驗成本、使決策發生錯誤的機率降低。  

## 實驗設計兩大部分  
1. 如何收集資料  
2. 如何分析收集到的資料  

## 單因子的設計  
Mean model： $y_{ij} = μ_i + ε_{ij}$
<span style="margin-left:2em">$i=1,2,3,4；k=1,2,3,4,5$ </span>   

Effect model： $y_{ij} = μ + τ_i + ε_{ij}$
<span style="margin-left:2em">$i=1,2,3,4；k=1,2,3,4,5$ </span>  
  
其中：$\epsilon_{ij} \sim \text{iid} \,\, \mathcal{N}(0, \sigma^2)$  
誤差項滿足：  
1. 常態性  
2. 獨立性  
3. 變異數同質性    

限制式(不只一種):  $Στ_i = 0$     
  
$Y_{ij} ：因子A在第i個水準下的第j個實驗的反應值$  
$μ_i ：因子A在第i個水準下的平均數$  
$ε_{ij} ：誤差項(外在因子)$  
$n_i : 因子A的在第i個水準下的樣本數$  


此報告中我們想研究如果當常態假設變成其他分配的時候，是否會影響 Effect Model ?  

- - -

# 基本 Code 介紹 (以常態分配為例)

## Model and Design Matrix
160 : Level 1  
180 : Level 2  
200 : Level 3  
220 : Level 4  

$$y_{ij}=\mu+\tau_i+\epsilon_{ij}$$
with the constraint that $\Sigma\tau_i=0, \, i=1,2,3,4$,  
$$-\tau_1=\tau_2+\tau_3+\tau_4$$

$$
\begin{bmatrix} -1 & -1 & -1 \\   1 & 0 & 0 \\  0 & 1 & 0 \\  0 & 0 & 1  \end{bmatrix} 
\begin{bmatrix} \tau_2\\  \tau_3\\  \tau_4  \end{bmatrix}
=
\begin{bmatrix}  -\tau_2-\tau_3-\tau_4 \\  \tau_2\\  \tau_3\\  \tau_4  \end{bmatrix}
=
\begin{bmatrix}  \tau_1 \\  \tau_2\\  \tau_3\\  \tau_4  \end{bmatrix}
$$


$$\hat{\mu}=\bar{y}_{..} \quad\quad \hat{\tau_1}=\bar{y}_{1.}-\bar{y}_{..}\quad\quad \hat{\tau_2}=\bar{y}_{2.}-\bar{y}_{..}\quad\quad  \hat{\tau_3}=\bar{y}_{3.}-\bar{y}_{..}\quad\quad \hat{\tau_4}=\bar{y}_{4.}-\bar{y}_{..}$$
```{r}
N <- 20 #樣本數
design_matrix <- function(N){
  levels <- c(rep(160, N/4), rep(180, N/4), rep(200, N/4), rep(220, N/4))
  levels <- as.factor(levels)
  X1 <- matrix(NA, N, 3) 
  for(i in 1:N){
    if(levels[i] == "160") {
      X1[i,] <- c(-1, -1, -1)
    }else if(levels[i] == "180"){
      X1[i,] <- c(1, 0, 0)
    }else if(levels[i] == "200"){
      X1[i,] <- c(0, 1, 0)
    }else if(levels[i] == "220"){
      X1[i,] <- c(0, 0, 1)
    }
  }
  return(X1)
}
X1 <- design_matrix(N=20)
```

## Hypothesis and ANOVA Table
Under $H_0 : \tau_1=\tau_2=\tau_3=\tau_4=0$.  
```{r fig.width=6, fig.height=4}
mu <- rep(617,N)
tau <- rep(0, N)
B <- 5000 #重複實驗次數
f <- rep(NA, B)
t <- 0

for (i in 1:B){
  error <- rnorm(N, 0, 1) 
  obs <- mu + tau + error
  fit <- lm(obs~X1)
  anova <- anova(fit)
  f[i] <- anova$`F value`[1]
  if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
}
par(bg="#fcfcfc")
plot(density(f), xlim=c(0,8),ylim=c(0.0,0.8), main="N(0, 1)", xlab="n")
curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1)
t/B # type I error
```

```{r}
anova
summary(fit)
```

## **Type I Error 應接近 0.05**
我們將 $\alpha$ 設置為 $0.05$，且最終出來的結果越接近 $0.05$ 越好  

若 $\alpha^* > 0.05$，拒絕域太大會太容易拒絕 $H_0$  
若 $\alpha^* < 0.05$，則不容易拒絕$H_0$  

## **Confidence Interval for Type I Error**
```{r}
ci_upper <- 0.05+1.96*sqrt(0.05*0.05/B)
ci_lower <- 0.05-1.96*sqrt(0.05*0.05/B)
ci_upper ; ci_lower
```

- - -

# **Poisson Distribution**

## lambda不變，控制樣本數
```{r fig.width=8, fig.height=5, warning=FALSE}
par(bg="#fcfcfc", mfrow=c(2, 2), mar=c(5, 4, 1.5, 1))
N <- c(16, 20, 36, 100) #樣本數
lambda <- 0.1

for (k in 1:length(N)) {
  X1 <- design_matrix(N[k])
  f <- rep(NA, B)
  t <- 0
  tau <- rep(0, N[k])
  mu <- rep(3, N[k])
  for (i in 1:B){
    error <- rpois(N[k], lambda) - lambda
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[i] <- anova$`F value`[1]
    if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
  }
  plot(density(f), xlim=c(0,10), ylim=c(0,0.8), main=paste0("n = ",N[k],", p-value = ",round(t/B,4)), xlab="n")
  curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
  legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1)
}
```

## 樣本數不變，控制lambda
```{r fig.width=8, fig.height=5, warning=FALSE}
par(bg="#fcfcfc", mfrow=c(2, 2), mar=c(5,4,1.5,1))
N <- 20
lambda <- c(0.1, 0.5, 1, 3)
p_value <- rep(NA, 4)

for (l in 1:length(lambda)) {
  X1 <- design_matrix(N)
  f <- rep(NA, B)
  t <- 0
  tau <- rep(0, N)
  mu <- rep(3, N)
  for (i in 1:B){
    error <- rpois(N, lambda[l]) - lambda[l]
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[i] <- anova$`F value`[1]
    if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
  }
  plot(density(f), xlim=c(0,10), ylim=c(0,0.8), main=paste0("lambda = ",lambda[l],", p-value = ",round(t/B,4)), xlab="n")
  curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
  legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1)
}
```

<img src='https://th.bing.com/th/id/OIP.z1BdIvfzsFZBn_60ybBRmgHaFe?pid=Api&rs=1'>
 
Poisson Distribution 在 lambda 小的時候 right-skewed  的情況會比較明顯；大的時候會比較接近常態分配，模擬出來的情況會比較好

- - -

# **Binomial Distribution**
## 試驗次數和p不變，控制樣本數
### n=12, 20, 100, 2000 ; p=0.25, size=100
```{r}
N <- c(12, 20, 100, 2000) #樣本數
par(bg="#fcfcfc", mfrow=c(2, 2), mar=c(5, 4, 1.5, 1))
for (k in 1:length(N)){
  X1 <- design_matrix(N[k])
  f <- rep(NA, B)
  t <- 0
  tau <- rep(0, N[k])
  mu <- rep(3, N[k])
  for (i in 1:B){
    error <- rbinom(N[k], size=100, 0.25) - 25
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[i] <- anova$`F value`[1]
    if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
  }
  plot(density(f), main=paste0("n = ",N[k],", p-value = ",round(t/B,4)), xlim=c(0,10), ylim=c(0,0.8), xlab="n")
  curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
  legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1.5)
}
```

### 改變實驗次數與機率
```{r warning=FALSE, fig.width=10, fig.height=7}
N <- 20
n <- c(5, 10, 20, 50) #實驗次數
p <- c(0.01, 0.05, 0.2, 0.5) #機率
X1 <- design_matrix(N)
par(bg="#fcfcfc", mfrow=c(4, 4), mar=c(1.7, 1.7, 1.5, 1.1))
f <- rep(NA, B)
for(s in 1:length(n)){
  for (k in 1:length(p)) {
    t <- 0
    tau <- rep(0, N)
    mu <- rep(3, N)
    for (i in 1:B){
      error <- rbinom(N, n[s], p[k]) - n[s]*p[k]
      obs <- mu + tau + error
      fit <- lm(obs~X1)
      anova <- anova(fit)
      f[i] <- anova$`F value`[1]
      if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
    }
    plot(density(f), main=paste0('Bin(', n[s],', ', p[k], ') , ', round(t/B,4)), xlim=c(0,10), ylim=c(0,0.8), xlab="n")
    curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
    #legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1.5)
   }
}
```

```{r warning=FALSE}
N <- 20 
n <- c(1, 5, 10, 30)
p <- c(0.005, 0.01, 0.05, 0.1, 0.3)
D <- expand.grid(N, n, p)
colnames(D) <- c('N','n','p')
D <- cbind(D, 'type_I_error'=rep(NA, dim(D)[1]))

for (i in 1:dim(D)[1]) {
  X1 <- design_matrix(N)
  f <- rep(NA, B)
  t <- 0
  tau <- rep(0, D$N[i])
  mu <- rep(3, D$N[i])
  for (j in 1:B) {
    error <- rbinom(D$N[i], D$n[i], D$p[i]) - D$n[i]*D$p[i]
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[j] <- anova$`F value`[1]
    if (!is.nan(anova$`F value`[1])){
      if (anova$`Pr(>F)`[1] <= 0.05) {t <- t+1}
    }
  }
  D$type_I_error[i] <- round(t/B, 4)
}
```

```{r warning=FALSE, echo=FALSE}
par(bg="#fcfcfc")
D_s <- split(D, D$n)
plot(D_s$`1`$p, D_s$`1`$type_I_error, main='', type='b', ylab='p-value', ylim=c(0,0.06), xlab='p')
lines(D_s$`5`$p, D_s$`5`$type_I_error, type='b', col='blue', axes=F, ylab='', xlab='')
lines(D_s$`10`$p, D_s$`10`$type_I_error, type='b', col='green3', axes=F, ylab='', xlab='')
lines(D_s$`30`$p, D_s$`30`$type_I_error, type='b', col='orange', axes=F, ylab='', xlab='')
abline(h=0.05, col="red")
legend("bottomright", c('n=1','n=5','n=10','n=30'), col=c(1,'blue','green3','orange'), lty=1)
```

# **Gamma Distribution**
## 控制樣本數
```{r}
#樣本數 = 8, 12, 16, 20
N <- c(8, 12, 16, 20)
alpha <- 1
beta <- 1
par(bg="#fcfcfc", mfrow=c(2, 2), mar=c(5, 4, 1.5, 1))

for (k in 1:length(N)) {
  X1 <- design_matrix(N[k])
  f <- rep(NA, B)
  t <- 0
  tau <- rep(0, N[k])
  mu <- rep(3, N[k])
  for (i in 1:B){
    error <- rgamma(N[k], alpha, beta) - alpha/beta
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[i] <- anova$`F value`[1]
    if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
  }
  plot(density(f), xlim=c(0,10), ylim=c(0,0.8), main=paste0("n = ",N[k],", p-value = ",round(t/B, 4)), xlab="")
  curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
  legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1)
}
```

## 看 alpha, beta 如何影響 pdf 圖形(n=100)
```{r fig.width=8, fig.height=6, echo=FALSE}
par(bg="#fcfcfc", mfrow=c(4,4), mar=c(3, 2, 1.5, 1.5))
alpha <- c(1/8,1/4,1,4)
beta <- c(1,2,3,4)
for(a in alpha){
    for(b in beta){
    f <- rgamma(100,a,b)-(a/(a+b))
    plot(density(f),main=paste0("alpha = ",a,", beta = ",b), xlab='')
    }
}
```
alpha 越大偏態越趨近於0（越對稱），觀察出Gamma分配的偏態係數$=2/\sqrt{\alpha}$的趨勢

## 調整 alpha 
```{r}
par(bg="#fcfcfc", mfrow=c(2, 2), mar=c(5,4,1.5,1))
N <- 12
t <- 0
alpha <- c(1/8,1/4,1,4)                             
beta <- 3
for (a in 1:length(alpha)) {
  X1 <- design_matrix(N)
  f <- rep(NA, B)
  t <- 0
  for (i in 1:B){
    error <- rgamma(N, alpha[a], beta)-(alpha[a]/beta)
    tau <- rep(0, N)
    mu <- rep(3, N)
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[i] <- anova$`F value`[1]
    if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
  }
  plot(density(f), xlim=c(0,10), ylim=c(0,0.8), main=paste0("alpha = ",alpha[a],", beta = ",beta ,", p-value = ",round(t/B,4)), xlab="")
  curve(df(x, anova$Df[1], anova$Df[1]), col=2, add=T)
  legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1)
}
```
-> 看圖可發現，固定beta之下，alpha越大，越趨近real F，且type-one error往0.05靠近

## 調整 beta 

```{r}
par(bg="#fcfcfc", mfrow=c(2, 2), mar=c(5,4,1.5,1))
N <- 20
t <- 0
alpha <- 1  
beta <- c(1,2,3,4)
for (b in 1:length(beta)) {
  X1 <- design_matrix(N)
  f <- rep(NA, B)
  t <- 0
  for (i in 1:B){
    error <- rgamma(N, alpha, beta[b])-(alpha/beta[b])
    tau <- rep(0, N)
    mu <- rep(3, N)
    obs <- mu + tau + error
    fit <- lm(obs~X1)
    anova <- anova(fit)
    f[i] <- anova$`F value`[1]
    if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
  }
  plot(density(f), xlim=c(0,10), ylim=c(0,0.8), main=paste0("alpha =", alpha, ", beta = ",beta[b],", p-value = ",round(t/B,4)), xlab="")
  curve(df(x, anova$Df[1], anova$Df[2]), col=2, add=T)
  legend("topright", c("simulation F", "real F"), col=c(1,2), lty=1)
}
```
beta 對於 f 沒有什麼顯著的發現


綜合以上，由於Gamma分配的偏態係數為 $2/\sqrt{\alpha}$ ，因此在alpha越大時，skewness會越趨近於0（越對稱），此時type-one error也較靠近0.05

- - -

# **t Distribution**
## 控制樣本數
### n = 8, 12, 16, 20, 40
```{r}
N <- c(8, 12, 16, 20, 40) 
p_value <- vector(mode="list",length=5)
for (k in 1:length(N)){
  X1 <- design_matrix(N[k])
  pp <- rep(NA, N[k]-1)
  for (df in 1:(N[k]-1)){
    t <- 0
    for (i in 1:B){
      error <- rt(N[k], df=df)
      tau <- rep(0, N[k])
      mu <- rep(3, N[k])
      obs <- mu + tau + error
      fit <- lm(obs~X1)
      anova <- anova(fit)
      if (anova$`Pr(>F)`[1]<=0.05) t <- t+1
    }
    pp[df] <- round(t/B,4)
  }
  p_value[[k]] <- pp
}
```

```{r echo=FALSE}
stat <- rep(NA, 5)
for (i in 1:5) {
  stat[i] <- mean(ifelse((p_value[[i]] < ci_upper) & (p_value[[i]] > ci_lower),1 ,0))
}
```

## 不同樣本數的 p-value 比較
### n=8
#### 在信賴區間的比例 `r stat[1]`

 Df=1 | Df=2 | Df=3 | Df=4 | Df=5 | Df=6 | Df=7 |
------|------|------|------|------|------|------|
`r p_value[[1]][1]`|`r p_value[[1]][2]`|`r p_value[[1]][3]`|`r p_value[[1]][4]`|`r p_value[[1]][5]`|`r p_value[[1]][6]`|`r p_value[[1]][7]`|


### n=12
#### 在信賴區間的比例 `r stat[2]`
 Df=1 | Df=2 | Df=3 | Df=4 | Df=5 | Df=6 | Df=7 | Df=8 | Df=9 | Df=10| Df=11|
------|------|------|------|------|------|------|------|------|------|------|
`r p_value[[2]][1]`|`r p_value[[2]][2]`|`r p_value[[2]][3]`|`r p_value[[2]][4]`|`r p_value[[2]][5]`|`r p_value[[2]][6]`|`r p_value[[2]][7]`|`r p_value[[2]][8]`|`r p_value[[2]][9]`|`r p_value[[2]][10]`|`r p_value[[2]][11]`|


### n=16
#### 在信賴區間的比例 `r stat[3]`
 Df=1 | Df=2 | Df=3 | Df=4 | Df=5 | Df=6 | Df=7 | Df=8 | Df=9 | Df=10|
------|------|------|------|------|------|------|------|------|------|
`r p_value[[3]][1]`|`r p_value[[3]][2]`|`r p_value[[3]][3]`|`r p_value[[3]][4]`|`r p_value[[3]][5]`|`r p_value[[3]][6]`|`r p_value[[3]][7]`|`r p_value[[3]][8]`|`r p_value[[3]][9]`|`r p_value[[3]][10]`|

Df=11 | Df=12 | Df=13 | Df=14 | Df=15 |
------|------|------|------|------|
`r p_value[[3]][11]`|`r p_value[[3]][12]`|`r p_value[[3]][13]`|`r p_value[[3]][14]`|`r p_value[[3]][15]`|


### n=20
#### 在信賴區間的比例 `r stat[4]`
 Df=1 | Df=2 | Df=3 | Df=4 | Df=5 | Df=6 | Df=7 | Df=8 | Df=9 | Df=10|
------|------|------|------|------|------|------|------|------|------|
`r p_value[[4]][1]`|`r p_value[[4]][2]`|`r p_value[[4]][3]`|`r p_value[[4]][4]`|`r p_value[[4]][5]`|`r p_value[[4]][6]`|`r p_value[[4]][7]`|`r p_value[[4]][8]`|`r p_value[[4]][9]`|`r p_value[[4]][10]`|

Df=11 | Df=12 | Df=13 | Df=14 | Df=15 | Df=16 | Df=17 | Df=18 | Df=19|
------|------|------|------|------|------|------|------|------|
`r p_value[[4]][11]` | `r p_value[[4]][12]` | `r p_value[[4]][13]` | `r p_value[[4]][14]` | `r p_value[[4]][15]` | `r p_value[[4]][16]` | `r p_value[[4]][17]` | `r p_value[[4]][18]` | `r p_value[[4]][19]` |


### n=40
#### 在信賴區間的比例 `r stat[5]`
 Df=1 | Df=2 | Df=3 | Df=4 | Df=5 | Df=6 | Df=7 | Df=8 | Df=9 | Df=10|
------|------|------|------|------|------|------|------|------|------|
`r p_value[[5]][1]` | `r p_value[[5]][2]` | `r p_value[[5]][3]` | `r p_value[[5]][4]` | `r p_value[[5]][5]` | `r p_value[[5]][6]` | `r p_value[[5]][7]` | `r p_value[[5]][8]` | `r p_value[[5]][9]` | `r p_value[[5]][10]` |

Df=11 | Df=12 | Df=13 | Df=14 | Df=15 | Df=16 | Df=17 | Df=18 | Df=19| Df=20|
------|------|------|------|------|------|------|------|------|------|
`r p_value[[5]][11]` | `r p_value[[5]][12]` | `r p_value[[5]][13]` | `r p_value[[5]][14]` | `r p_value[[5]][15]` | `r p_value[[5]][16]` | `r p_value[[5]][17]` | `r p_value[[5]][18]` | `r p_value[[5]][19]` | `r p_value[[5]][20]` |

 Df=21 | Df=22 | Df=23 | Df=24 | Df=25 | Df=26 | Df=27 | Df=28 | Df=29| Df=30|
------|------|------|------|------|------|------|------|------|------|
`r p_value[[5]][21]` | `r p_value[[5]][22]` | `r p_value[[5]][23]` | `r p_value[[5]][24]` | `r p_value[[5]][25]` | `r p_value[[5]][26]` | `r p_value[[5]][27]` | `r p_value[[5]][28]` | `r p_value[[5]][29]` | `r p_value[[5]][30]` |

Df=31 | Df=32 | Df=33 | Df=34 | Df=35 | Df=36 | Df=37 | Df=38 | Df=39|
------|------|------|------|------|------|------|------|------|
`r p_value[[5]][31]` | `r p_value[[5]][32]` | `r p_value[[5]][33]` | `r p_value[[5]][34]` | `r p_value[[5]][35]` | `r p_value[[5]][36]` | `r p_value[[5]][37]` | `r p_value[[5]][38]` | `r p_value[[5]][39]` |

```{r fig.width=10, fig.height=5, warning=FALSE, echo=FALSE}
par(bg="#fcfcfc")
plot(sequence(N[1]-1), p_value[[1]], main='', type='b', ylab='p-value', ylim=c(0,0.06), xlab='df', xlim=c(1,20))
lines(sequence(N[2]-1), p_value[[2]], type='b', col='green3', axes=F, ylab='', xlab='')
lines(sequence(N[3]-1), p_value[[3]], type='b', col='orange', axes=F, ylab='', xlab='')
lines(sequence(N[4]-1), p_value[[4]], type='b', col='blue', axes=F, ylab='', xlab='')

abline(h=0.05, col="red",lwd=2)
legend("bottomright", paste0('N=',N[1:4]), col=c('blue','orange','green3',1), lty=1.5)
```

樣本數越大，越有機率涵蓋真正的type I error 值，而自由度不要太大也不要太小時，會越有可能越接近0.05

- - -

# 結論




